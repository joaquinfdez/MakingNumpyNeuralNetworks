{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.875 · Deep Learning · PEC1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2018-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PEC 1: Redes neuronales completamente conectadas\n",
    "\n",
    "En esta práctica implementaremos una red neuronal completamente conectada de dos formas diferentes: \n",
    "\n",
    "<ol start=\"1\">\n",
    "  <li>Partiendo de cero utilizando únicamente la librería numpy</li>\n",
    "  <li>Utilizando la librería Keras y TensorFlow</li>\n",
    "</ol>\n",
    "\n",
    "Posteriormente utilizaremos las dos implementaciones para entrenar dos redes neuronales iguales en un conjunto de datos y compararemos el rendimiento.\n",
    "\n",
    "**Importante: Cada uno de los ejercicios puede suponer varios minutos de ejecución, por lo que la entrega debe hacerse en formato notebook y en formato html donde se vea el código y los resultados y comentarios de cada ejercicio. Para exportar el notebook a html puede hacerse desde el menú File $\\to$ Download as $\\to$ HTML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Carga de datos\n",
    "\n",
    "El siguiente código carga los paquetes necesarios para la práctica y además lee los datos que utilizaremos para entrenar la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "features = data[\"features\"]\n",
    "labels = data[\"labels\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Redes neuronales utilizando numpy \n",
    "\n",
    "A continuación implementaremos todas las funciones necesarias para entrenar una red neuronal completamente conectada utilizando únicamente la librería numpy. El objetivo es poder entrenar una red neuronal con cualquier número de capas en la cual la última capa tendrá una única neurona con función de activación sigmoid y las demás capas cualquier número de neuronas con función de activación relu.\n",
    "\n",
    "La siguiente figura muestra un diagrama de como implementaremos el proceso de entrenamiento de la red neuronal:\n",
    "\n",
    "<img src=\"diag.png\" alt=\"Diagrama del entrenamiento de la red neuronal\" style=\"height: 550px;\"/>\n",
    "\n",
    "El desarrollo está estructurado en funciones básicas que se componen según el siguiente esquema:\n",
    "\n",
    "- L_layer_model\n",
    "  - initialize_parameters\n",
    "  - L_model_forward\n",
    "    - linear_activation_forward\n",
    "      - linear_forward\n",
    "      - sigmoid\n",
    "      - relu\n",
    "  - compute_cost\n",
    "  - L_model_backward\n",
    "    - linear_activation_backward\n",
    "      - linear_backward\n",
    "      - sigmoid_backward\n",
    "      - relu_backward\n",
    "  - update_parameters\n",
    "- accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notación**:\n",
    "- Denotamos $L$ el número de capas de la red neuronal.\n",
    "- La matriz de pesos que conecta una capa con la siguiente la denotamos con la letra $W$, mientras que el vector de bias lo denotamos con la letra $b$.\n",
    "- Superíndice $[l]$ denota una cantidad asociada con la capa número $l$. \n",
    "    - Ejemplo: $a^{[L]}$ denota la salida de la capa número $L$.\n",
    "    - Ejemplo: Las variables $W^{[L]}$ y $b^{[L]}$ denotan la matriz de pesos y el vector de bias que conectan la capa $L-1$ con la capa $L$ respectivamente.\n",
    "- Superíndice $(i)$ denota una cantidad asociada con el ejemplo $i$-ésimo. \n",
    "    - Ejemplo: $x^{(i)}$ es el ejemplo del conjunto de entrenamiento número $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Inicialización de parámetros\n",
    "\n",
    "El primer paso para entrenar una red neuronal consiste en inicializar de forma aleatoria los parámetros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Inicializar las matrices de parámetros y los vectores de bias. Las matrices de pesos se deben inicializar utilizando la distribución normal y los vectores de bias se deben inicializar con ceros. Para las matrices de pesos podéis utilizar 0.1*np.random.randn(shape) indicando el tamaño correcto.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    layer_dims -- lista que contiene las dimensiones de cada capa de la red\n",
    "    \n",
    "    Devuelve:\n",
    "    parameters -- diccionario que contiene los parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- matriz de pesos de tamaño (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- vector de bias de tamaño (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    #print(\"Nº de parametros: \", L)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        #print(\"Capa de entrada: \", layer_dims[l-1], \"capa de salida\", layer_dims[l])\n",
    "        parameters['W' + str(l)] = 0.1 * np.random.randn(layer_dims[l], layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = 0.1 * np.random.randn( layer_dims[l], 1)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Propagación hacia delante\n",
    "\n",
    "En una capa concreta de una red neuronal, las entradas de las neuronas se combinan de forma lineal antes de pasar por la función de activación según la siguiente fórmula:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular la combinación lineal de las entradas a una capa de la red neuronal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implementa la parte lineal de la propagación hacia delante de una capa\n",
    "\n",
    "    Argumentos:\n",
    "    A -- salida de la capa anterior (o datos de entrada): (número de neuronas de la capa anterior, número de ejemplos)\n",
    "    W -- matriz de pesos: (número de neuronas de la capa actual, número de neuronas de la capa anterior)\n",
    "    b -- vector de bias: (número de neuronas de la capa actual, 1)\n",
    "\n",
    "    Devuelve:\n",
    "    Z -- la entrada a la función de activación\n",
    "    cache -- una tripleta que contiene \"A\", \"W\" y \"b\", utilizada después para la propagación hacia atrás\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    #print(\"W shape\", W.shape, \" A shape \", A.shape, \" b shape\", b.shape)\n",
    "    #print(\"linear_forward; Z=\", Z)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se ha calculado la combinación lineal de las entradas de una capa se debe aplicar una función de activación no lineal antes de enviar las salidas a la siguiente capa. Si denotamos $g$ la función de activación (en nuestro caso relu o sigmoid), tenemos la siguiente fórmula:\n",
    "\n",
    "$$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} + b^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos las funciones de activación que utilizaremos en la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular la combinación lineal de las entradas utilizando la función implementada anteriormente y aplicar la función de activación no lineal que corresponda.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia delante de una capa incluyendo la función de activación\n",
    "\n",
    "    Argumentos:\n",
    "    A_prev -- salida de la capa anterior (o datos de entrada): \n",
    "                (número de neuronas de la capa anterior, número de ejemplos)\n",
    "    W -- matriz de pesos: (número de neuronas de la capa actual, número de neuronas de la capa anterior)\n",
    "    b -- vector de bias: (número de neuronas de la capa actual, 1)\n",
    "    activation -- el nombre de la función de activación a utilizar en la capa: \"sigmoid\" o \"relu\"\n",
    "\n",
    "    Devuelve:\n",
    "    A -- la salida de la capa después de aplicar la función de activación\n",
    "    cache -- una dupla que contiene \"linear_cache\" y \"activation_cache\", utilizada después para la propagación hacia atrás\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":    \n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados los datos de entrada, la salida de la red neuronal se calcula aplicando diferentes capas una detrás de otra. Si denotamos la última capa como $L$, la salida de la red neuronal se corresponde con la salida de la última capa $A^{[L]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular la salida de la red neuronal aplicando $L-1$ capas con función de activación relu y una última capa con función de activación sigmoid.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia delante de la red neuronal completa\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- datos: matriz de tamaño (número de variables, número de ejemplos)\n",
    "    parameters -- salida de la función initialize_parameters()\n",
    "    \n",
    "    Devuelve:\n",
    "    AL -- salida de la red neuronal\n",
    "    caches -- lista de caches que contiene todas las caches de la función linear_activation_forward(), las caches\n",
    "                indexadas de 0 a L-2 corresponden a las caches de la función de activación relu y la cache indexada\n",
    "                como L-1 corresponde a la cache de la función de activación sigmoid\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Implementa primero las L-1 capas con función de activación relu\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        \n",
    "        activation = \"relu\"\n",
    "        W_current = parameters['W' + str(l)]\n",
    "        b_current = parameters['b' + str(l)]\n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev, W_current, b_current, activation)\n",
    "        \n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implementa la última capa con función de activación sigmoid\n",
    "    activation = \"sigmoid\"\n",
    "    W_current = parameters['W' + str(L)]\n",
    "    b_current = parameters['b' + str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W_current, b_current, activation)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Función de coste\n",
    "\n",
    "Una vez hemos obtenido la salida de la red neuronal podemos obtener un valor que mida el rendimiento de la red neuronal utilizando una función de coste $\\mathcal{L}$. En nuestro caso utilizaremos la función de coste log-loss, que viene definida por la siguiente fórmula:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular el valor de la función de coste log-loss dada la salida de la red neuronal junto con las etiquetas correctas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Calcula la función de coste\n",
    "\n",
    "    Argumentos:\n",
    "    AL -- vector que contiene la salida de la red, corresponde a las probabilidades que predice la red neuronal\n",
    "            para cada ejemplo: (1, número de ejemplos)\n",
    "    Y -- vector con las etiquetas correctas para los datos de entrada a la red: (1, número de ejemplos)\n",
    "\n",
    "    Devuelve:\n",
    "    cost -- valor de la función de coste log-loss\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = -1 / m * (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Propagación hacia atrás\n",
    "\n",
    "Para entrenar una red neuronal es necesario calcular el gradiente de la función de coste repescto a los parámetros de la red, para lo cual utilizaremos la propagación hacia atrás. La propagación hacia atrás consiste en aplicar la regla de la cadena para calcular el gradiente de la función de coste paso a paso en cada capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar la regla de la cadena en la parte lineal de la neurona, supongamos que ya hemos calculado la derivada $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Entonces, para calcular las derivadas $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ podemos utilizar las siguientes fórmulas:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Calcular las derivadas de la parte lineal para una sola capa.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implementa la parte lineal de la propagación hacia atrás para una única capa\n",
    "\n",
    "    Argumentos:\n",
    "    dZ -- derivada de la función de coste con respecto a la salida lineal de la capa actual\n",
    "    cache -- tripleta que contiene los valores (A_prev, W, b), provinientes de la función linear_forward\n",
    "\n",
    "    Devuelve:\n",
    "    dA_prev -- derivada de la función de coste con respecto a la salida de la capa anterior (l-1): \n",
    "                tiene el mismo tamaño que A_prev\n",
    "    dW -- derivada de la función de coste con respecto a la matriz de pesos W de la capa actual (l):\n",
    "                tiene el mismo tamaño que W\n",
    "    db -- derivada de la función de coste con respecto al vector de bias b de la capa actual (l):\n",
    "                tiene el mismo tamaño que b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1 / m * (np.dot(dZ, A_prev.T))\n",
    "    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso consiste en aplicar la regla de la cadena a la parte no lineal de las neuronas, es decir, a las funciones de activación. Para esto, si denotamos $g$ la función de activación, podemos utilizar la siguiente fórmula:\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "\n",
    "Donde $*$ indica el producto componente a componente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación calculamos las derivadas de las funciones de activación que utilizamos en la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Combinar el cálculo de la derivada de las funciones de activación con la derivada de la parte lineal para obtener, a partir de la derivada de la función de coste respecto la activación de una capa, la derivada de la función de coste respecto a los parámetros de la capa y respecto a las activaciones de la capa anterior.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una única capa incluyendo la función de activación\n",
    "    \n",
    "    Argumentos:\n",
    "    dA -- derivada de la función de coste con respecto a la salida de la capa actual (l)\n",
    "    cache -- dupla que contiene \"linear_cache\" y \"activation_cache\", provinientes de la función linear_activation_forward\n",
    "    activation -- el nombre de la función de activación utilizada en la capa actual (l): \"sigmoid\" o \"relu\"\n",
    "    \n",
    "    Devuelve:\n",
    "    dA_prev -- derivada de la función de coste con respecto a la salida de la capa anterior (l-1):\n",
    "                tiene el mismo tamaño que A_prev\n",
    "    dW -- derivada de la función de coste con respecto a la matriz de pesos W de la capa actual (l):\n",
    "                tiene el mismo tamaño que W\n",
    "    db -- derivada de la función de coste con respecto al vector de bias b de la capa actual (l):\n",
    "                tiene el mismo tamaño que b\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    #print(\"linear_activation_backward:Z=\", activation_cache.shape)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, es posible calcular la derivada de la función de coste respecto a cualquiera de los parámetros aplicando las funciones recién implementadas empezando por la última capa. Observemos que para inicializar la propagación hacia atrás es necesario calcular primero el valor de $\\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Aplicar la propagación hacia atrás para calcular el gradiente de la función de coste. Observad que el valor de $\\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$ viene calculado en la variable dAL y que la última capa tiene función de activación sigmoid mientras que todas las demás tienen función de activación relu.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de la red neuronal completa\n",
    "    \n",
    "    Argumentos:\n",
    "    AL -- salida de la red neuronal, proviene de la función L_model_forward\n",
    "    Y -- vector con las etiquetas correctas para cada ejemplo del conjunto de datos: (1, número de ejemplos)\n",
    "    caches -- lista de caches que contiene todas las caches de la función linear_activation_forward(), las caches\n",
    "                indexadas de 0 a L-2 corresponden a las caches de la función de activación relu y la cache indexada\n",
    "                como L-1 corresponde a la cache de la función de activación sigmoid\n",
    "    \n",
    "    Devuelve:\n",
    "    grads -- Un diccionario con las derivadas de la función de coste respecto de cada variable:\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Inicialización de la propagación hacia atrás\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Gradiente de la última capa\n",
    "    current_cache = caches[L-1]\n",
    "    \n",
    "    activation = \"sigmoid\"\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation)\n",
    "    \n",
    "    grads[\"dA\" + str(L)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    activation = \"relu\"\n",
    "    # Gradiente de las capas restantes\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l] \n",
    "        \n",
    "        dA_curr = dA_prev_temp\n",
    "        \n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_curr, current_cache, activation)\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Actualización de parámetros\n",
    "\n",
    "Una vez disponemos del gradiente de la función de coste podemos utilizar el método del descenso del gradiente para actualizar los parámetros de la red neuronal. Si denotamos $\\alpha$ la velocidad de aprendizaje, las fórmulas para aplicar un paso del descenso del gradiente son:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Actualizar los parámetros de la red neuronal aplicando un paso del descenso del gradiente.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Actualiza los parámetros utilizando el descenso del gradiente\n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- diccionario que contiene los parámetros de la red neuronal\n",
    "    grads -- diccionario con las derivadas de la función de coste respecto a cada parámetro,\n",
    "                corresponde a la salida de la función L_model_backward\n",
    "    \n",
    "    Devuelve:\n",
    "    parameters -- diccionario con los parámetros actualizados:\n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con todo esto es posible entrenar la red neuronal combinando las funciones definidas anteriormente para aplicar diversas iteraciones del descenso del gradiente e ir actualizando los parámetros de la red de forma reiterada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra cómo entrenar la red neuronal que hemos construido utilizando únicamente la librería numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, print_cost):\n",
    "    \"\"\"\n",
    "    Implementa una red neuronal de L capas donde las L-1 primeras capas tienen función de activación relu y \n",
    "    la última capa tiene función de activación sigmoid.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- datos: matriz de tamaño (número de variables, número de ejemplos)\n",
    "    Y -- vector con las etiquetas correctas para cada ejemplo del conjunto de datos: (1, número de ejemplos)\n",
    "    layers_dims -- lista de longitud (número de capas + 1) que contiene el número de variables y el número \n",
    "                    de neuronas en cada capa, \n",
    "    learning_rate -- velocidad de aprendizaje para aplicar el método del descenso del gradiente\n",
    "    num_iterations -- número de pasos para aplicar el descenso del gradiente\n",
    "    print_cost -- si el valor es True, escribe el valor de la función de coste cada 10 iteraciones\n",
    "    \n",
    "    Devuelve:\n",
    "    parameters -- parámetros ajustados de la red neuronal\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicialización de los parámetros\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        # Propagación hacia delante\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Cálculo de la función de coste\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Propagación hacia atrás\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Actualización de parámetros\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Escribe el valor de la función de coste cada 10 iteraciones\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redes neuronales utilizando Keras\n",
    "\n",
    "A continuación definiremos una red neuronal completamente conectada igual a la que hemos implementado anteriormente pero esta vez utilizando la librería Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Definir una red neuronal completamente conectada a partir de una lista que contiene el número de neuronas que debe tener cada capa de la red. Las primeras capas deben tener función de activación relu y la última capa debe tener función de activación sigmoid. Todas ellas tienen que tener kernel_initializer=\"random_normal\" y bias_initializer=\"zeros\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def keras_model(layers_dims, learning_rate):\n",
    "    \"\"\"\n",
    "    Crea, utilizando Keras, una red neuronal de L capas completamente conectadas donde las L-1 primeras capas\n",
    "    tienen función de activación relu y la última capa tiene función de activación sigmoid.\n",
    "    \n",
    "    Argumentos:\n",
    "    layers_dims -- lista de longitud (número de capas + 1) que contiene el número de variables y el número \n",
    "                    de neuronas en cada capa, \n",
    "    learning_rate -- velocidad de aprendizaje para aplicar el método del descenso del gradiente\n",
    "    \n",
    "    Devuelve:\n",
    "    modelo -- objeto de Keras que representa la red neuronal\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layers_dims)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Añadir L-1 capas con función de activación relu y una última capa con función de activación sigmoid,\n",
    "    # cada capa debe tener el número de neuronas indicado en la variable layers_dims, el tamaño de la capa\n",
    "    # de entrada viene dado en layers_dims[0]\n",
    "    for l in range(L-2):\n",
    "        model.add(Dense(layers_dims[l], activation = 'relu'))\n",
    "        \n",
    "    model.add(Dense(layers_dims[L-1], activation = 'sigmoid'))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento de la red neuronal\n",
    "\n",
    "Con todas las funciones implementadas anteriormente es posible entrenar una red neuronal completamente conectada con cualquier número de capas y cualquier número de neuronas en cada capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos la estructura de capas que tendrá la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [100, 80, 50, 40, 20, 5, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar la red neuronal que hemos construido únicamente utilizando numpy debemos ejecutar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.695712\n",
      "Cost after iteration 10: 0.694522\n",
      "Cost after iteration 20: 0.693761\n",
      "Cost after iteration 30: 0.693244\n",
      "Cost after iteration 40: 0.692860\n",
      "Cost after iteration 50: 0.692537\n",
      "Cost after iteration 60: 0.692231\n",
      "Cost after iteration 70: 0.691904\n",
      "Cost after iteration 80: 0.691525\n",
      "Cost after iteration 90: 0.691052\n",
      "Cost after iteration 100: 0.690431\n",
      "Cost after iteration 110: 0.689565\n",
      "Cost after iteration 120: 0.688282\n",
      "Cost after iteration 130: 0.686319\n",
      "Cost after iteration 140: 0.683359\n",
      "Cost after iteration 150: 0.678627\n",
      "Cost after iteration 160: 0.670799\n",
      "Cost after iteration 170: 0.657658\n",
      "Cost after iteration 180: 0.633955\n",
      "Cost after iteration 190: 0.587569\n",
      "Cost after iteration 200: 0.497403\n",
      "Cost after iteration 210: 0.357097\n",
      "Cost after iteration 220: 0.212702\n",
      "Cost after iteration 230: 0.124110\n",
      "Cost after iteration 240: 0.087445\n"
     ]
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x.T, train_y.reshape(1, -1), layers_dims=layers_dims, learning_rate=0.1, \n",
    "                           num_iterations=250, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar la red neuronal que hemos construido utilizando Keras debemos ejecutar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      " - 1s - loss: 0.7099 - acc: 0.4350\n",
      "Epoch 2/250\n",
      " - 0s - loss: 0.6738 - acc: 0.7056\n",
      "Epoch 3/250\n",
      " - 0s - loss: 0.6453 - acc: 0.8062\n",
      "Epoch 4/250\n",
      " - 0s - loss: 0.6174 - acc: 0.8456\n",
      "Epoch 5/250\n",
      " - 0s - loss: 0.5870 - acc: 0.8681\n",
      "Epoch 6/250\n",
      " - 0s - loss: 0.5522 - acc: 0.8788\n",
      "Epoch 7/250\n",
      " - 0s - loss: 0.5153 - acc: 0.8894\n",
      "Epoch 8/250\n",
      " - 0s - loss: 0.4763 - acc: 0.8988\n",
      "Epoch 9/250\n",
      " - 0s - loss: 0.4360 - acc: 0.9044\n",
      "Epoch 10/250\n",
      " - 0s - loss: 0.3965 - acc: 0.9137\n",
      "Epoch 11/250\n",
      " - 0s - loss: 0.3584 - acc: 0.9200\n",
      "Epoch 12/250\n",
      " - 0s - loss: 0.3236 - acc: 0.9250\n",
      "Epoch 13/250\n",
      " - 0s - loss: 0.2926 - acc: 0.9294\n",
      "Epoch 14/250\n",
      " - 0s - loss: 0.2658 - acc: 0.9325\n",
      "Epoch 15/250\n",
      " - 0s - loss: 0.2427 - acc: 0.9350\n",
      "Epoch 16/250\n",
      " - 0s - loss: 0.2229 - acc: 0.9356\n",
      "Epoch 17/250\n",
      " - 0s - loss: 0.2060 - acc: 0.9375\n",
      "Epoch 18/250\n",
      " - 0s - loss: 0.1917 - acc: 0.9400\n",
      "Epoch 19/250\n",
      " - 0s - loss: 0.1795 - acc: 0.9419\n",
      "Epoch 20/250\n",
      " - 0s - loss: 0.1690 - acc: 0.9425\n",
      "Epoch 21/250\n",
      " - 0s - loss: 0.1598 - acc: 0.9438\n",
      "Epoch 22/250\n",
      " - 0s - loss: 0.1517 - acc: 0.9450\n",
      "Epoch 23/250\n",
      " - 0s - loss: 0.1446 - acc: 0.9463\n",
      "Epoch 24/250\n",
      " - 0s - loss: 0.1382 - acc: 0.9513\n",
      "Epoch 25/250\n",
      " - 0s - loss: 0.1324 - acc: 0.9525\n",
      "Epoch 26/250\n",
      " - 0s - loss: 0.1271 - acc: 0.9538\n",
      "Epoch 27/250\n",
      " - 0s - loss: 0.1222 - acc: 0.9550\n",
      "Epoch 28/250\n",
      " - 0s - loss: 0.1176 - acc: 0.9575\n",
      "Epoch 29/250\n",
      " - 0s - loss: 0.1134 - acc: 0.9575\n",
      "Epoch 30/250\n",
      " - 0s - loss: 0.1094 - acc: 0.9569\n",
      "Epoch 31/250\n",
      " - 0s - loss: 0.1057 - acc: 0.9581\n",
      "Epoch 32/250\n",
      " - 0s - loss: 0.1022 - acc: 0.9594\n",
      "Epoch 33/250\n",
      " - 0s - loss: 0.0989 - acc: 0.9613\n",
      "Epoch 34/250\n",
      " - 0s - loss: 0.0958 - acc: 0.9619\n",
      "Epoch 35/250\n",
      " - 0s - loss: 0.0929 - acc: 0.9631\n",
      "Epoch 36/250\n",
      " - 0s - loss: 0.0901 - acc: 0.9631\n",
      "Epoch 37/250\n",
      " - 0s - loss: 0.0875 - acc: 0.9638\n",
      "Epoch 38/250\n",
      " - 0s - loss: 0.0849 - acc: 0.9669\n",
      "Epoch 39/250\n",
      " - 0s - loss: 0.0826 - acc: 0.9675\n",
      "Epoch 40/250\n",
      " - 0s - loss: 0.0803 - acc: 0.9681\n",
      "Epoch 41/250\n",
      " - 0s - loss: 0.0782 - acc: 0.9694\n",
      "Epoch 42/250\n",
      " - 0s - loss: 0.0761 - acc: 0.9719\n",
      "Epoch 43/250\n",
      " - 0s - loss: 0.0742 - acc: 0.9725\n",
      "Epoch 44/250\n",
      " - 0s - loss: 0.0724 - acc: 0.9731\n",
      "Epoch 45/250\n",
      " - 0s - loss: 0.0707 - acc: 0.9737\n",
      "Epoch 46/250\n",
      " - 0s - loss: 0.0691 - acc: 0.9737\n",
      "Epoch 47/250\n",
      " - 0s - loss: 0.0676 - acc: 0.9756\n",
      "Epoch 48/250\n",
      " - 0s - loss: 0.0661 - acc: 0.9762\n",
      "Epoch 49/250\n",
      " - 0s - loss: 0.0647 - acc: 0.9775\n",
      "Epoch 50/250\n",
      " - 0s - loss: 0.0634 - acc: 0.9775\n",
      "Epoch 51/250\n",
      " - 0s - loss: 0.0621 - acc: 0.9775\n",
      "Epoch 52/250\n",
      " - 0s - loss: 0.0609 - acc: 0.9781\n",
      "Epoch 53/250\n",
      " - 0s - loss: 0.0598 - acc: 0.9787\n",
      "Epoch 54/250\n",
      " - 0s - loss: 0.0587 - acc: 0.9800\n",
      "Epoch 55/250\n",
      " - 0s - loss: 0.0577 - acc: 0.9800\n",
      "Epoch 56/250\n",
      " - 0s - loss: 0.0567 - acc: 0.9806\n",
      "Epoch 57/250\n",
      " - 0s - loss: 0.0557 - acc: 0.9825\n",
      "Epoch 58/250\n",
      " - 0s - loss: 0.0548 - acc: 0.9825\n",
      "Epoch 59/250\n",
      " - 0s - loss: 0.0539 - acc: 0.9825\n",
      "Epoch 60/250\n",
      " - 0s - loss: 0.0530 - acc: 0.9825\n",
      "Epoch 61/250\n",
      " - 0s - loss: 0.0522 - acc: 0.9825\n",
      "Epoch 62/250\n",
      " - 0s - loss: 0.0513 - acc: 0.9825\n",
      "Epoch 63/250\n",
      " - 0s - loss: 0.0506 - acc: 0.9825\n",
      "Epoch 64/250\n",
      " - 0s - loss: 0.0498 - acc: 0.9825\n",
      "Epoch 65/250\n",
      " - 0s - loss: 0.0491 - acc: 0.9825\n",
      "Epoch 66/250\n",
      " - 0s - loss: 0.0484 - acc: 0.9825\n",
      "Epoch 67/250\n",
      " - 0s - loss: 0.0477 - acc: 0.9825\n",
      "Epoch 68/250\n",
      " - 0s - loss: 0.0470 - acc: 0.9825\n",
      "Epoch 69/250\n",
      " - 0s - loss: 0.0464 - acc: 0.9819\n",
      "Epoch 70/250\n",
      " - 0s - loss: 0.0457 - acc: 0.9831\n",
      "Epoch 71/250\n",
      " - 0s - loss: 0.0451 - acc: 0.9831\n",
      "Epoch 72/250\n",
      " - 0s - loss: 0.0445 - acc: 0.9831\n",
      "Epoch 73/250\n",
      " - 0s - loss: 0.0439 - acc: 0.9831\n",
      "Epoch 74/250\n",
      " - 0s - loss: 0.0433 - acc: 0.9831\n",
      "Epoch 75/250\n",
      " - 0s - loss: 0.0427 - acc: 0.9831\n",
      "Epoch 76/250\n",
      " - 0s - loss: 0.0422 - acc: 0.9837\n",
      "Epoch 77/250\n",
      " - 0s - loss: 0.0416 - acc: 0.9837\n",
      "Epoch 78/250\n",
      " - 0s - loss: 0.0411 - acc: 0.9844\n",
      "Epoch 79/250\n",
      " - 0s - loss: 0.0406 - acc: 0.9844\n",
      "Epoch 80/250\n",
      " - 0s - loss: 0.0401 - acc: 0.9850\n",
      "Epoch 81/250\n",
      " - 0s - loss: 0.0396 - acc: 0.9850\n",
      "Epoch 82/250\n",
      " - 0s - loss: 0.0391 - acc: 0.9850\n",
      "Epoch 83/250\n",
      " - 0s - loss: 0.0386 - acc: 0.9856\n",
      "Epoch 84/250\n",
      " - 0s - loss: 0.0382 - acc: 0.9862\n",
      "Epoch 85/250\n",
      " - 0s - loss: 0.0377 - acc: 0.9862\n",
      "Epoch 86/250\n",
      " - 0s - loss: 0.0373 - acc: 0.9869\n",
      "Epoch 87/250\n",
      " - 0s - loss: 0.0369 - acc: 0.9875\n",
      "Epoch 88/250\n",
      " - 0s - loss: 0.0364 - acc: 0.9875\n",
      "Epoch 89/250\n",
      " - 0s - loss: 0.0360 - acc: 0.9881\n",
      "Epoch 90/250\n",
      " - 0s - loss: 0.0356 - acc: 0.9881\n",
      "Epoch 91/250\n",
      " - 0s - loss: 0.0352 - acc: 0.9881\n",
      "Epoch 92/250\n",
      " - 0s - loss: 0.0348 - acc: 0.9887\n",
      "Epoch 93/250\n",
      " - 0s - loss: 0.0344 - acc: 0.9887\n",
      "Epoch 94/250\n",
      " - 0s - loss: 0.0341 - acc: 0.9894\n",
      "Epoch 95/250\n",
      " - 0s - loss: 0.0337 - acc: 0.9894\n",
      "Epoch 96/250\n",
      " - 0s - loss: 0.0333 - acc: 0.9894\n",
      "Epoch 97/250\n",
      " - 0s - loss: 0.0330 - acc: 0.9894\n",
      "Epoch 98/250\n",
      " - 0s - loss: 0.0326 - acc: 0.9894\n",
      "Epoch 99/250\n",
      " - 0s - loss: 0.0323 - acc: 0.9894\n",
      "Epoch 100/250\n",
      " - 0s - loss: 0.0319 - acc: 0.9894\n",
      "Epoch 101/250\n",
      " - 0s - loss: 0.0316 - acc: 0.9894\n",
      "Epoch 102/250\n",
      " - 0s - loss: 0.0313 - acc: 0.9894\n",
      "Epoch 103/250\n",
      " - 0s - loss: 0.0309 - acc: 0.9894\n",
      "Epoch 104/250\n",
      " - 0s - loss: 0.0306 - acc: 0.9894\n",
      "Epoch 105/250\n",
      " - 0s - loss: 0.0303 - acc: 0.9894\n",
      "Epoch 106/250\n",
      " - 0s - loss: 0.0299 - acc: 0.9894\n",
      "Epoch 107/250\n",
      " - 0s - loss: 0.0296 - acc: 0.9894\n",
      "Epoch 108/250\n",
      " - 0s - loss: 0.0293 - acc: 0.9894\n",
      "Epoch 109/250\n",
      " - 0s - loss: 0.0290 - acc: 0.9894\n",
      "Epoch 110/250\n",
      " - 0s - loss: 0.0287 - acc: 0.9900\n",
      "Epoch 111/250\n",
      " - 0s - loss: 0.0284 - acc: 0.9900\n",
      "Epoch 112/250\n",
      " - 0s - loss: 0.0281 - acc: 0.9900\n",
      "Epoch 113/250\n",
      " - 0s - loss: 0.0278 - acc: 0.9900\n",
      "Epoch 114/250\n",
      " - 0s - loss: 0.0275 - acc: 0.9906\n",
      "Epoch 115/250\n",
      " - 0s - loss: 0.0272 - acc: 0.9906\n",
      "Epoch 116/250\n",
      " - 0s - loss: 0.0269 - acc: 0.9906\n",
      "Epoch 117/250\n",
      " - 0s - loss: 0.0266 - acc: 0.9906\n",
      "Epoch 118/250\n",
      " - 0s - loss: 0.0263 - acc: 0.9906\n",
      "Epoch 119/250\n",
      " - 0s - loss: 0.0260 - acc: 0.9906\n",
      "Epoch 120/250\n",
      " - 0s - loss: 0.0258 - acc: 0.9906\n",
      "Epoch 121/250\n",
      " - 0s - loss: 0.0255 - acc: 0.9906\n",
      "Epoch 122/250\n",
      " - 0s - loss: 0.0252 - acc: 0.9912\n",
      "Epoch 123/250\n",
      " - 0s - loss: 0.0250 - acc: 0.9912\n",
      "Epoch 124/250\n",
      " - 0s - loss: 0.0247 - acc: 0.9912\n",
      "Epoch 125/250\n",
      " - 0s - loss: 0.0244 - acc: 0.9912\n",
      "Epoch 126/250\n",
      " - 0s - loss: 0.0242 - acc: 0.9912\n",
      "Epoch 127/250\n",
      " - 0s - loss: 0.0239 - acc: 0.9912\n",
      "Epoch 128/250\n",
      " - 0s - loss: 0.0237 - acc: 0.9912\n",
      "Epoch 129/250\n",
      " - 0s - loss: 0.0234 - acc: 0.9912\n",
      "Epoch 130/250\n",
      " - 0s - loss: 0.0231 - acc: 0.9919\n",
      "Epoch 131/250\n",
      " - 0s - loss: 0.0229 - acc: 0.9919\n",
      "Epoch 132/250\n",
      " - 0s - loss: 0.0227 - acc: 0.9919\n",
      "Epoch 133/250\n",
      " - 0s - loss: 0.0224 - acc: 0.9925\n",
      "Epoch 134/250\n",
      " - 0s - loss: 0.0222 - acc: 0.9925\n",
      "Epoch 135/250\n",
      " - 0s - loss: 0.0219 - acc: 0.9925\n",
      "Epoch 136/250\n",
      " - 0s - loss: 0.0217 - acc: 0.9925\n",
      "Epoch 137/250\n",
      " - 0s - loss: 0.0215 - acc: 0.9925\n",
      "Epoch 138/250\n",
      " - 0s - loss: 0.0212 - acc: 0.9925\n",
      "Epoch 139/250\n",
      " - 0s - loss: 0.0210 - acc: 0.9925\n",
      "Epoch 140/250\n",
      " - 0s - loss: 0.0208 - acc: 0.9925\n",
      "Epoch 141/250\n",
      " - 0s - loss: 0.0206 - acc: 0.9925\n",
      "Epoch 142/250\n",
      " - 0s - loss: 0.0204 - acc: 0.9925\n",
      "Epoch 143/250\n",
      " - 0s - loss: 0.0201 - acc: 0.9925\n",
      "Epoch 144/250\n",
      " - 0s - loss: 0.0199 - acc: 0.9925\n",
      "Epoch 145/250\n",
      " - 0s - loss: 0.0197 - acc: 0.9925\n",
      "Epoch 146/250\n",
      " - 0s - loss: 0.0195 - acc: 0.9925\n",
      "Epoch 147/250\n",
      " - 0s - loss: 0.0193 - acc: 0.9925\n",
      "Epoch 148/250\n",
      " - 0s - loss: 0.0191 - acc: 0.9925\n",
      "Epoch 149/250\n",
      " - 0s - loss: 0.0189 - acc: 0.9925\n",
      "Epoch 150/250\n",
      " - 0s - loss: 0.0187 - acc: 0.9925\n",
      "Epoch 151/250\n",
      " - 0s - loss: 0.0185 - acc: 0.9925\n",
      "Epoch 152/250\n",
      " - 0s - loss: 0.0183 - acc: 0.9931\n",
      "Epoch 153/250\n",
      " - 0s - loss: 0.0181 - acc: 0.9944\n",
      "Epoch 154/250\n",
      " - 0s - loss: 0.0179 - acc: 0.9944\n",
      "Epoch 155/250\n",
      " - 0s - loss: 0.0177 - acc: 0.9944\n",
      "Epoch 156/250\n",
      " - 0s - loss: 0.0176 - acc: 0.9944\n",
      "Epoch 157/250\n",
      " - 0s - loss: 0.0174 - acc: 0.9944\n",
      "Epoch 158/250\n",
      " - 0s - loss: 0.0172 - acc: 0.9944\n",
      "Epoch 159/250\n",
      " - 0s - loss: 0.0170 - acc: 0.9944\n",
      "Epoch 160/250\n",
      " - 0s - loss: 0.0168 - acc: 0.9944\n",
      "Epoch 161/250\n",
      " - 0s - loss: 0.0167 - acc: 0.9950\n",
      "Epoch 162/250\n",
      " - 0s - loss: 0.0165 - acc: 0.9950\n",
      "Epoch 163/250\n",
      " - 0s - loss: 0.0163 - acc: 0.9950\n",
      "Epoch 164/250\n",
      " - 0s - loss: 0.0162 - acc: 0.9956\n",
      "Epoch 165/250\n",
      " - 0s - loss: 0.0160 - acc: 0.9956\n",
      "Epoch 166/250\n",
      " - 0s - loss: 0.0158 - acc: 0.9956\n",
      "Epoch 167/250\n",
      " - 0s - loss: 0.0157 - acc: 0.9956\n",
      "Epoch 168/250\n",
      " - 0s - loss: 0.0155 - acc: 0.9956\n",
      "Epoch 169/250\n",
      " - 0s - loss: 0.0153 - acc: 0.9956\n",
      "Epoch 170/250\n",
      " - 0s - loss: 0.0152 - acc: 0.9956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/250\n",
      " - 0s - loss: 0.0150 - acc: 0.9956\n",
      "Epoch 172/250\n",
      " - 0s - loss: 0.0149 - acc: 0.9956\n",
      "Epoch 173/250\n",
      " - 0s - loss: 0.0147 - acc: 0.9956\n",
      "Epoch 174/250\n",
      " - 0s - loss: 0.0145 - acc: 0.9956\n",
      "Epoch 175/250\n",
      " - 0s - loss: 0.0144 - acc: 0.9956\n",
      "Epoch 176/250\n",
      " - 0s - loss: 0.0142 - acc: 0.9956\n",
      "Epoch 177/250\n",
      " - 0s - loss: 0.0141 - acc: 0.9956\n",
      "Epoch 178/250\n",
      " - 0s - loss: 0.0139 - acc: 0.9956\n",
      "Epoch 179/250\n",
      " - 0s - loss: 0.0138 - acc: 0.9956\n",
      "Epoch 180/250\n",
      " - 0s - loss: 0.0136 - acc: 0.9956\n",
      "Epoch 181/250\n",
      " - 0s - loss: 0.0135 - acc: 0.9956\n",
      "Epoch 182/250\n",
      " - 0s - loss: 0.0134 - acc: 0.9956\n",
      "Epoch 183/250\n",
      " - 0s - loss: 0.0132 - acc: 0.9962\n",
      "Epoch 184/250\n",
      " - 0s - loss: 0.0131 - acc: 0.9962\n",
      "Epoch 185/250\n",
      " - 0s - loss: 0.0129 - acc: 0.9962\n",
      "Epoch 186/250\n",
      " - 0s - loss: 0.0128 - acc: 0.9962\n",
      "Epoch 187/250\n",
      " - 0s - loss: 0.0127 - acc: 0.9962\n",
      "Epoch 188/250\n",
      " - 0s - loss: 0.0125 - acc: 0.9962\n",
      "Epoch 189/250\n",
      " - 0s - loss: 0.0124 - acc: 0.9962\n",
      "Epoch 190/250\n",
      " - 0s - loss: 0.0123 - acc: 0.9962\n",
      "Epoch 191/250\n",
      " - 0s - loss: 0.0121 - acc: 0.9962\n",
      "Epoch 192/250\n",
      " - 0s - loss: 0.0120 - acc: 0.9962\n",
      "Epoch 193/250\n",
      " - 0s - loss: 0.0119 - acc: 0.9962\n",
      "Epoch 194/250\n",
      " - 0s - loss: 0.0118 - acc: 0.9962\n",
      "Epoch 195/250\n",
      " - 0s - loss: 0.0116 - acc: 0.9962\n",
      "Epoch 196/250\n",
      " - 0s - loss: 0.0115 - acc: 0.9962\n",
      "Epoch 197/250\n",
      " - 0s - loss: 0.0114 - acc: 0.9962\n",
      "Epoch 198/250\n",
      " - 0s - loss: 0.0113 - acc: 0.9962\n",
      "Epoch 199/250\n",
      " - 0s - loss: 0.0112 - acc: 0.9962\n",
      "Epoch 200/250\n",
      " - 0s - loss: 0.0110 - acc: 0.9962\n",
      "Epoch 201/250\n",
      " - 0s - loss: 0.0109 - acc: 0.9962\n",
      "Epoch 202/250\n",
      " - 0s - loss: 0.0108 - acc: 0.9962\n",
      "Epoch 203/250\n",
      " - 0s - loss: 0.0107 - acc: 0.9962\n",
      "Epoch 204/250\n",
      " - 0s - loss: 0.0106 - acc: 0.9975\n",
      "Epoch 205/250\n",
      " - 0s - loss: 0.0105 - acc: 0.9975\n",
      "Epoch 206/250\n",
      " - 0s - loss: 0.0104 - acc: 0.9975\n",
      "Epoch 207/250\n",
      " - 0s - loss: 0.0103 - acc: 0.9975\n",
      "Epoch 208/250\n",
      " - 0s - loss: 0.0102 - acc: 0.9975\n",
      "Epoch 209/250\n",
      " - 0s - loss: 0.0100 - acc: 0.9981\n",
      "Epoch 210/250\n",
      " - 0s - loss: 0.0099 - acc: 0.9987\n",
      "Epoch 211/250\n",
      " - 0s - loss: 0.0098 - acc: 0.9987\n",
      "Epoch 212/250\n",
      " - 0s - loss: 0.0097 - acc: 0.9987\n",
      "Epoch 213/250\n",
      " - 0s - loss: 0.0096 - acc: 0.9987\n",
      "Epoch 214/250\n",
      " - 0s - loss: 0.0095 - acc: 0.9987\n",
      "Epoch 215/250\n",
      " - 0s - loss: 0.0094 - acc: 0.9987\n",
      "Epoch 216/250\n",
      " - 0s - loss: 0.0093 - acc: 0.9987\n",
      "Epoch 217/250\n",
      " - 0s - loss: 0.0092 - acc: 0.9987\n",
      "Epoch 218/250\n",
      " - 0s - loss: 0.0091 - acc: 0.9987\n",
      "Epoch 219/250\n",
      " - 0s - loss: 0.0091 - acc: 0.9987\n",
      "Epoch 220/250\n",
      " - 0s - loss: 0.0090 - acc: 0.9987\n",
      "Epoch 221/250\n",
      " - 0s - loss: 0.0089 - acc: 0.9994\n",
      "Epoch 222/250\n",
      " - 0s - loss: 0.0088 - acc: 0.9994\n",
      "Epoch 223/250\n",
      " - 0s - loss: 0.0087 - acc: 0.9994\n",
      "Epoch 224/250\n",
      " - 0s - loss: 0.0086 - acc: 0.9994\n",
      "Epoch 225/250\n",
      " - 0s - loss: 0.0085 - acc: 0.9994\n",
      "Epoch 226/250\n",
      " - 0s - loss: 0.0084 - acc: 0.9994\n",
      "Epoch 227/250\n",
      " - 0s - loss: 0.0083 - acc: 0.9994\n",
      "Epoch 228/250\n",
      " - 0s - loss: 0.0082 - acc: 0.9994\n",
      "Epoch 229/250\n",
      " - 0s - loss: 0.0082 - acc: 0.9994\n",
      "Epoch 230/250\n",
      " - 0s - loss: 0.0081 - acc: 0.9994\n",
      "Epoch 231/250\n",
      " - 0s - loss: 0.0080 - acc: 0.9994\n",
      "Epoch 232/250\n",
      " - 0s - loss: 0.0079 - acc: 0.9994\n",
      "Epoch 233/250\n",
      " - 0s - loss: 0.0078 - acc: 0.9994\n",
      "Epoch 234/250\n",
      " - 0s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 235/250\n",
      " - 0s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 236/250\n",
      " - 0s - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 237/250\n",
      " - 0s - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 238/250\n",
      " - 0s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 239/250\n",
      " - 0s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 240/250\n",
      " - 0s - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 241/250\n",
      " - 0s - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 242/250\n",
      " - 0s - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 243/250\n",
      " - 0s - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 244/250\n",
      " - 0s - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 245/250\n",
      " - 0s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 246/250\n",
      " - 0s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 247/250\n",
      " - 0s - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 248/250\n",
      " - 0s - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 249/250\n",
      " - 0s - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 250/250\n",
      " - 0s - loss: 0.0066 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd2264a8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras_model(layers_dims = layers_dims, learning_rate = 0.1)\n",
    "model.fit(train_x, train_y, epochs=250, batch_size=train_x.shape[0], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, podemos utilizar la siguiente función para calcular la precisión que obtenemos con la red neuronal construida utilizando únicamente numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, parameters):\n",
    "    \"\"\"\n",
    "    Calcula la precisión de las predicciones de la red neuronal.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- datos: matriz de tamaño (número de variables, número de ejemplos)\n",
    "    parameters -- parámetros de la red neuronal entrenada\n",
    "    \n",
    "    Returns:\n",
    "    accuracy -- valor entre 0 y 1 que representa la precisión de la red neuronal\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Propagación hacia delante\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    # Conversión de la salida de la red a valores 0 o 1\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0, i] > 0.5:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "            \n",
    "    accuracy = np.sum((p == y)) / m\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la precisión obtenida tanto con la red construida con numpy como con la red construida con Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red construida con numpy\n",
      "Precisión 0.96\n",
      "---\n",
      "Red construida con Keras\n",
      "Precisión 0.98\n"
     ]
    }
   ],
   "source": [
    "print(\"Red construida con numpy\")\n",
    "print(\"Precisión {:.2f}\".format(accuracy(test_x.T, test_y.reshape(1, -1), parameters)))\n",
    "print(\"---\")\n",
    "print(\"Red construida con Keras\")\n",
    "print(\"Precisión {:.2f}\".format(model.evaluate(test_x, test_y, verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código permite calcular el tiempo que tarda cada red neuronal en entrenarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.72 s ± 307 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "parameters = L_layer_model(train_x.T, train_y.reshape(1, -1), layers_dims=layers_dims, \n",
    "                           learning_rate=0.1, num_iterations=250, print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.09 s ± 239 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model = keras_model(layers_dims=layers_dims, learning_rate=0.1)\n",
    "model.fit(train_x, train_y, epochs=250, batch_size=train_x.shape[0], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis:</strong> Comparar el rendimiento, tanto en tiempo de ejecución como en precisión, de las dos implementaciones de la red neuronal. Utilizar diferentes hiperparámetros en la comparación: probar con diferentes valores para las dimensiones de las capas, diferente número de capas, número de iteraciones, etc. ¿Qué factores pueden estar creando las diferencias observadas?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C7FCE5; border-color:#9DFCD2  ; border-left: 5px solid #9DFCD2  ; padding: 0.5em;\">\n",
    "    <p>A continuación vamos a visualizar un conjunto de tablas las cuales reflejamos la modificación de los diferentes parámetros como pueden ser <b>el número de iteraciones y la tasa de aprendizaje</b> y visualizamos los resultados en función de <b> la precisión y el tiempo de entrenamiento</b>.</p>\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>layers_dims = [100, 20, 5, 1]</b>\n",
    "\n",
    "<b>Numpy</b>\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "  <td><strong>learning_rate</strong></td>\n",
    "  <td><strong>num_iterations</strong></td>\n",
    "  <td><strong>accuracy</strong></td>\n",
    "  <td><strong>time(s)</strong></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>50</td>\n",
    "  <td>0.87</td>\n",
    "  <td>0.152</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>50</td>\n",
    "  <td>0.98</td>\n",
    "  <td>0.153</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>250</td>\n",
    "  <td>0.98</td>\n",
    "  <td>0.742</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>250</td>\n",
    "  <td>0.99</td>\n",
    "  <td>0.750</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>500</td>\n",
    "  <td>0.99</td>\n",
    "  <td>1.54</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>500</td>\n",
    "  <td>0.99</td>\n",
    "  <td>1.5</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "<b>Keras</b>\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "  <td><strong>learning_rate</strong></td>\n",
    "  <td><strong>num_iterations</strong></td>\n",
    "  <td><strong>accuracy</strong></td>\n",
    "  <td><strong>time(s)</strong></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>50</td>\n",
    "  <td>0.97</td>\n",
    "  <td>3.69</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>50</td>\n",
    "  <td>0.99</td>\n",
    "  <td>3.54</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>250</td>\n",
    "  <td>0.99</td>\n",
    "  <td>2.35</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>250</td>\n",
    "  <td>0.99</td>\n",
    "  <td>4.68</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>500</td>\n",
    "  <td>0.99</td>\n",
    "  <td>6.12</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>500</td>\n",
    "  <td>0.99</td>\n",
    "  <td>5.5</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<b>layers_dims = [100, 80, 50, 40, 20, 5, 1]</b>\n",
    "\n",
    "<b>Numpy</b>\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "  <td><strong>learning_rate</strong></td>\n",
    "  <td><strong>num_iterations</strong></td>\n",
    "  <td><strong>accuracy</strong></td>\n",
    "  <td><strong>time(s)</strong></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>50</td>  \n",
    "  <td>0.48</td>\n",
    "  <td>4.69</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>50</td>\n",
    "  <td>0.80</td>\n",
    "  <td>3.57</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>250</td>\n",
    "  <td>0.98</td>\n",
    "  <td>4.69</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>250</td>\n",
    "  <td>0.98</td>\n",
    "  <td>3.06</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>10</td>\n",
    "  <td>250</td>\n",
    "  <td>0.48</td>\n",
    "  <td>3.31</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>500</td>\n",
    "  <td>0.99</td>\n",
    "  <td>3.54</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>500</td>\n",
    "  <td>0.99</td>\n",
    "  <td>3.41</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "<b>Keras</b>\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "  <td><strong>learning_rate</strong></td>\n",
    "  <td><strong>num_iterations</strong></td>\n",
    "  <td><strong>accuracy</strong></td>\n",
    "  <td><strong>time(s)</strong></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>50</td>  \n",
    "  <td>0.96</td>\n",
    "  <td>6.55</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>50</td>  \n",
    "  <td>0.52</td>\n",
    "  <td>7.8</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>250</td>\n",
    "  <td>0.99</td>\n",
    "  <td>6.55</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>250</td>\n",
    "  <td>0.99</td>\n",
    "  <td>7.06</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>10</td>\n",
    "  <td>250</td>\n",
    "  <td>0.48</td>\n",
    "  <td>8.01</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.1</td>\n",
    "  <td>500</td>\n",
    "  <td>0.99</td>\n",
    "  <td>6.55</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "  <td>0.5</td>\n",
    "  <td>500</td>\n",
    "  <td>0.99</td>\n",
    "  <td>7.02</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C7FCE5; border-color:#9DFCD2  ; border-left: 5px solid #9DFCD2  ; padding: 0.5em;\">\n",
    "    <h4>Precisión</h4>\n",
    "    <p>Centrándonos en la precisión, <b>ambos modelos de redes neuronales obtienen resultados parecidos en la mayoría de pruebas realizadas pero por el contrario se observa que numpy consigue unos tiempos de entrenamiento más rápidos que Keras</b>. Esto se debe a que <b>Keras implementa funciones más complejas en comparación a las funciones que hemos creado con Numpy</b> y, aunque esto proporciona a Keras que escale mejor para ejemplos más complejos, <b>provoca un mayor tiempo de aprendizaje del modelo</b>.</p>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C7FCE5; border-color:#9DFCD2  ; border-left: 5px solid #9DFCD2  ; padding: 0.5em;\">\n",
    "    <h4>Velocidad de aprendizaje</h4>\n",
    "    <p>La <b>velocidad de aprendizaje es un parámetro que controla cuánto estamos ajustando los pesos de nuestra red neuronal respecto al gradiente</b>. Cuanto más bajo sea el valor más lento correrán los valores dentro de la red. Aunque pueda parecer que una velocidad de aprendizaje baja puede conseguir mejores resultados, puede producir el efecto contrario ya que tiene el riesgo de caer en un mínimo local y tomar mucho tiempo para converger.</p>\n",
    "    <p>Asímismo, ajustar la velocidad de aprendizaje en el proceso de entrenamiento es un factor muy importante para obtener una buena precisión. De esta manera, hemos aplicado los valores <b>10, 0,5 y 0,1</b> de tasa de aprendizaje sobre <b>numpy</b> y <b>keras</b>. Es fácil comprobar que para altos valores de este parámetro, como el valor 10, provoca una inestabilidad en la red y de hecho obtiene una precisión del 0,48, por el contrario los valores 0,5 y 0,1 funcionan mejor que el parámetro anterior. Sin embargo, tampoco se observa que para un valor muy bajo de velocidad de aprendizaje como lo es el 0,1, en el caso de numpy, no termina de converger. </p>\n",
    "    <p>De esta manera, <b>parece ser que es interesante utilizar el valor intermedio de 0,5 ya que consigue converger mejor que con los otros parámetros.</b></p>\n",
    "    <img src=\"learning-rate-graphic.png\" alt=\"Diagrama del entrenamiento de la red neuronal\"/>\n",
    "<i>Gráfica de la comparación de diferentes parámetros de velocidad de aprendizaje para una red neuronal con las siguientes capas: [100, 80, 50, 40, 20, 5, 1]</i>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C7FCE5; border-color:#9DFCD2  ; border-left: 5px solid #9DFCD2  ; padding: 0.5em;\">\n",
    "    <h4>Iteraciones (epoch)</h4>\n",
    "    <p>En redes neuronales es necesario pasar el conjunto de datos varias veces a la misma red neuronal para su entrenamiento. El problema es que a priori es difícil determinar el valor óptimo para conseguir un mejor aprendizaje. Lo ideal es detener el número de iteraciones cuando el algoritmo se estanca y no consigue mejorar a continuación.</p>   \n",
    "    <p>En nuestro caso hemos hecho pruebas con los valores de 50 iter., 250 iter. y 500 iteraciones. Es fácil darse cuenta que a partir de las 250 iteraciones el modelo ya no consigue mejores resultados y puede provocar un posible efecto de sobreaprendizaje. Por otro lado, 50 iteraciones son pocas y provoca resultado de precisión más bajos tanto en Keras como en Numpy.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C7FCE5; border-color:#9DFCD2  ; border-left: 5px solid #9DFCD2  ; padding: 0.5em;\">\n",
    "    <h4>Capas</h4>\n",
    "    <p>Añadiendo capas y neuronas a la red conseguimos un aumento de predicción y capacidad de separación de los datos en su clasificación pero por el contrario podemos provocar una sobreespecialización y un aumento del coste computacional en tiempo de entrenamiento.</p>\n",
    "    <p>Se ha probado con una arquitectura de 3 capas <b>layers_dims = [100, 20, 5, 1]</b> y con <b>layers_dims = [100, 80, 50, 40, 20, 5, 1] una de 6 capas.</b> y por lo general podemos decir que se ha aumentado el tiempo de aprendizaje tanto en Keras como en Numpy. Respecto a la precisión, no podemos afirmar conseguir una mejora en comparación con la primera arquitectura por lo que quizás no nos interesa esta ya que puede generar sobreaprendizaje.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "Parte del codigo utilizado para desarrollar esta práctica proviene del curso de Coursera [\"Neural networks and deep learning\"](https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
